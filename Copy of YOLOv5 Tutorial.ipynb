{"cells":[{"cell_type":"markdown","metadata":{"id":"t6MPjfT5NrKQ"},"source":["\u003ca align=\"left\" href=\"https://ultralytics.com/yolov5\" target=\"_blank\"\u003e\n","\u003cimg src=\"https://user-images.githubusercontent.com/26833433/125273437-35b3fc00-e30d-11eb-9079-46f313325424.png\"\u003e\u003c/a\u003e\n","\n","This is the **official YOLOv5 ðŸš€ notebook** by **Ultralytics**, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n","For more information please visit https://github.com/ultralytics/yolov5 and https://ultralytics.com. Thank you!"]},{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["# Setup\n","\n","Clone repo, install dependencies and check PyTorch and GPU."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10692,"status":"ok","timestamp":1630028481456,"user":{"displayName":"Luke Skywalker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIHDrPlGzlezX_meIfyxjCB6yHjbu-xGx7jeU5MeI=s64","userId":"17975635846773368728"},"user_tz":-540},"id":"wbvMlHd_QwMG","outputId":"49c39292-1461-4e50-c6b1-858ced491771"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setup complete. Using torch 1.9.0+cu102 (Tesla K80)\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt  # install dependencies\n","\n","import torch\n","from IPython.display import Image, clear_output  # to display images\n","\n","clear_output()\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"]},{"cell_type":"markdown","metadata":{"id":"4JnkELT0cIJg"},"source":["# 1. Inference\n","\n","`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n","\n","```shell\n","python detect.py --source 0  # webcam\n","                          file.jpg  # image \n","                          file.mp4  # video\n","                          path/  # directory\n","                          path/*.jpg  # glob\n","                          'https://youtu.be/NUsoVlDFqZg'  # YouTube\n","                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n","```"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15951,"status":"ok","timestamp":1630028497402,"user":{"displayName":"Luke Skywalker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIHDrPlGzlezX_meIfyxjCB6yHjbu-xGx7jeU5MeI=s64","userId":"17975635846773368728"},"user_tz":-540},"id":"zR9ZbuQCH7FX","outputId":"e2ef9d20-c676-4bfb-fd71-b0f86af05792"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images/, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n","YOLOv5 ðŸš€ v5.0-380-g11f85e7 torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00\u003c00:00, 15.9MB/s]\n","\n","Fusing layers... \n","Model Summary: 224 layers, 7266973 parameters, 0 gradients\n","image 1/2 /content/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 1 fire hydrant, Done. (0.060s)\n","image 2/2 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, Done. (0.027s)\n","Results saved to \u001b[1mruns/detect/exp\u001b[0m\n","Done. (0.328s)\n"]}],"source":["%rm -rf runs\n","!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\n","#Image(filename='runs/detect/exp/zidane.jpg', width=600)"]},{"cell_type":"markdown","metadata":{"id":"hkAzDWJ7cWTr"},"source":["\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n","\u003cimg align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\"\u003e"]},{"cell_type":"markdown","metadata":{"id":"0eq1SMWl6Sfn"},"source":["# 2. Validate\n","Validate a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."]},{"cell_type":"markdown","metadata":{"id":"eyTZYGgRjnMc"},"source":["## COCO val2017\n","Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"elapsed":39555,"status":"ok","timestamp":1630028536944,"user":{"displayName":"Luke Skywalker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIHDrPlGzlezX_meIfyxjCB6yHjbu-xGx7jeU5MeI=s64","userId":"17975635846773368728"},"user_tz":-540},"id":"WQPtK1QYVaD_","outputId":"57092504-6a5e-40f3-e541-dbc052fc7d9d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96d67aa61d5e4d25a01d144b6461f27a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/780M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Download COCO val2017\n","torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n","!unzip -q tmp.zip -d ../datasets \u0026\u0026 rm tmp.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"X58w8JLpMnjH"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=./data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\n","YOLOv5 ðŸš€ v5.0-380-g11f85e7 torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt to yolov5x.pt...\n","100% 168M/168M [00:08\u003c00:00, 21.9MB/s]\n","\n","Fusing layers... \n","Model Summary: 476 layers, 87730285 parameters, 0 gradients\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:02\u003c00:00, 1987.49it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../datasets/coco/val2017.cache\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [10:01\u003c00:00,  3.83s/it]\n","                 all       5000      36335      0.746      0.626       0.68       0.49\n","Speed: 0.2ms pre-process, 111.9ms inference, 1.7ms NMS per image at shape (32, 3, 640, 640)\n","\n","Evaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\n","loading annotations into memory...\n","Done (t=0.45s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=5.26s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=99.14s).\n","Accumulating evaluation results...\n","DONE (t=14.26s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n","Results saved to \u001b[1mruns/val/exp\u001b[0m\n"]}],"source":["# Run YOLOv5x on COCO val2017\n","!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half"]},{"cell_type":"markdown","metadata":{"id":"rc_KbFk0juX2"},"source":["## COCO test-dev2017\n","Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0AJnSeCIHyJ"},"outputs":[],"source":["# Download COCO test-dev2017\n","torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n","!unzip -q tmp.zip -d ../ \u0026\u0026 rm tmp.zip # unzip labels\n","!f=\"test2017.zip\" \u0026\u0026 curl http://images.cocodataset.org/zips/$f -o $f \u0026\u0026 unzip -q $f \u0026\u0026 rm $f  # 7GB,  41k images\n","%mv ./test2017 ../coco/images  # move to /coco"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29GJXAP_lPrt"},"outputs":[],"source":["# Run YOLOv5s on COCO test-dev2017 using --task test\n","!python val.py --weights yolov5s.pt --data coco.yaml --task test"]},{"cell_type":"markdown","metadata":{"id":"VUOiNLtMP5aG"},"source":["# 3. Train\n","\n","Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Knxi2ncxWffW"},"outputs":[],"source":["# Download COCO128\n","torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n","!unzip -q tmp.zip -d ../datasets \u0026\u0026 rm tmp.zip"]},{"cell_type":"markdown","metadata":{"id":"_pOkGLv1dMqh"},"source":["Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n","\n","All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOy5KI2ncnWd"},"outputs":[],"source":["# Tensorboard  (optional)\n","%load_ext tensorboard\n","%tensorboard --logdir runs/train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fLAV42oNb7M"},"outputs":[],"source":["# Weights \u0026 Biases  (optional)\n","%pip install -q wandb\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NcFxRcFdJ_O"},"outputs":[],"source":["# Train YOLOv5s on COCO128 for 3 epochs\n","!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"]},{"cell_type":"markdown","metadata":{"id":"15glLzbQx5u0"},"source":["# 4. Visualize"]},{"cell_type":"markdown","metadata":{"id":"DLI1JmHU7B0l"},"source":["## Weights \u0026 Biases Logging ðŸŒŸ NEW\n","\n","[Weights \u0026 Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W\u0026B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W\u0026B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n","\n","During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights \u0026 Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n","\n","\u003cimg align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/125274843-a27bc600-e30e-11eb-9a44-62af0b7a50a2.png\" width=\"800\"\u003e"]},{"cell_type":"markdown","metadata":{"id":"-WPvRbS5Swl6"},"source":["## Local Logging\n","\n","All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and val jpgs to see mosaics, labels, predictions and augmentation effects. Note an Ultralytics **Mosaic Dataloader** is used for training (shown below), which combines 4 images into 1 mosaic during training.\n","\n","\u003e \u003cimg src=\"https://user-images.githubusercontent.com/26833433/124931219-48bf8700-e002-11eb-84f0-e05d95b118dd.jpg\" width=\"700\"\u003e  \n","`train_batch0.jpg` shows train batch 0 mosaics and labels\n","\n","\u003e \u003cimg src=\"https://user-images.githubusercontent.com/26833433/124931217-4826f080-e002-11eb-87b9-ae0925a8c94b.jpg\" width=\"700\"\u003e  \n","`test_batch0_labels.jpg` shows val batch 0 labels\n","\n","\u003e \u003cimg src=\"https://user-images.githubusercontent.com/26833433/124931209-46f5c380-e002-11eb-9bd5-7a3de2be9851.jpg\" width=\"700\"\u003e  \n","`test_batch0_pred.jpg` shows val batch 0 _predictions_\n","\n","Training results are automatically logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) as `results.csv`, which is plotted as `results.png` (below) after training completes. You can also plot any `results.csv` file manually:\n","\n","```python\n","from utils.plots import plot_results \n","plot_results('path/to/results.csv')  # plot 'results.csv' as 'results.png'\n","```\n","\n","\u003cimg align=\"left\" width=\"800\" alt=\"COCO128 Training Results\" src=\"https://user-images.githubusercontent.com/26833433/126906780-8c5e2990-6116-4de6-b78a-367244a33ccf.png\"\u003e"]},{"cell_type":"markdown","metadata":{"id":"Zelyeqbyt3GD"},"source":["# Environments\n","\n","YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n","\n","- **Google Colab and Kaggle** notebooks with free GPU: \u003ca href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"\u003e\u003c/a\u003e \u003ca href=\"https://www.kaggle.com/ultralytics/yolov5\"\u003e\u003cimg src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"\u003e\u003c/a\u003e\n","- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n","- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n","- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) \u003ca href=\"https://hub.docker.com/r/ultralytics/yolov5\"\u003e\u003cimg src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","metadata":{"id":"6Qu7Iesl0p54"},"source":["# Status\n","\n","![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n","\n","If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"]},{"cell_type":"markdown","metadata":{"id":"IEijrePND_2I"},"source":["# Appendix\n","\n","Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcKoSIK2WSzj"},"outputs":[],"source":["# Reproduce\n","for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n","  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n","  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMusP4OAxFu6"},"outputs":[],"source":["# PyTorch Hub\n","import torch\n","\n","# Model\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n","\n","# Images\n","dir = 'https://ultralytics.com/images/'\n","imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n","\n","# Inference\n","results = model(imgs)\n","results.print()  # or .show(), .save()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGH0ZjkGjejy"},"outputs":[],"source":["# Unit tests\n","%%shell\n","export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n","\n","rm -rf runs  # remove runs/\n","for m in yolov5s; do  # models\n","  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n","  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n","  for d in 0 cpu; do  # devices\n","    python detect.py --weights $m.pt --device $d  # detect official\n","    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n","    python val.py --weights $m.pt --device $d # val official\n","    python val.py --weights runs/train/exp/weights/best.pt --device $d # val custom\n","  done\n","  python hubconf.py  # hub\n","  python models/yolo.py --cfg $m.yaml  # inspect\n","  python export.py --weights $m.pt --img 640 --batch 1  # export\n","done"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gogI-kwi3Tye"},"outputs":[],"source":["# Profile\n","from utils.torch_utils import profile\n","\n","m1 = lambda x: x * torch.sigmoid(x)\n","m2 = torch.nn.SiLU()\n","results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVRSOhEvUdb5"},"outputs":[],"source":["# Evolve\n","!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n","!d=runs/train/evolve \u0026\u0026 cp evolve.* $d \u0026\u0026 zip -r evolve.zip $d \u0026\u0026 gsutil mv evolve.zip gs://bucket  # upload results (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSgFCAcMbk1R"},"outputs":[],"source":["# VOC\n","for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n","  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of YOLOv5 Tutorial","provenance":[{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1630028197639}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0566340acd4642a8aebe586f5167c07f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b62c28a4b27d4d629a303214e000d8ce","placeholder":"â€‹","style":"IPY_MODEL_ba81998abd7742e897d67fb52c8930d1","value":" 780M/780M [00:31\u0026lt;00:00, 28.9MB/s]"}},"35f4b5b217794a39ae2c263928480f05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_efa3e1e4a77a4335949fda7501e6ea04","max":818322941,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44a9b7a9d12541c98bb81d87b23c7402","value":818322941}},"44a9b7a9d12541c98bb81d87b23c7402":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ff14c8f63e84b28a472de9f0b1738f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91fa581641fe49a6a80ebf2eaf779b5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96d67aa61d5e4d25a01d144b6461f27a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7c8fc1854bf4c9f80608d27ef4b2f66","IPY_MODEL_35f4b5b217794a39ae2c263928480f05","IPY_MODEL_0566340acd4642a8aebe586f5167c07f"],"layout":"IPY_MODEL_4ff14c8f63e84b28a472de9f0b1738f3"}},"b62c28a4b27d4d629a303214e000d8ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba81998abd7742e897d67fb52c8930d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7c8fc1854bf4c9f80608d27ef4b2f66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91fa581641fe49a6a80ebf2eaf779b5e","placeholder":"â€‹","style":"IPY_MODEL_e23ce38d2cad46dca3938d741c7be906","value":"100%"}},"e23ce38d2cad46dca3938d741c7be906":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efa3e1e4a77a4335949fda7501e6ea04":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}